{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X1fX6kz8PuM5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "from moviepy.editor import VideoFileClip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR2P8zD9Yq5p"
   },
   "source": [
    "### Image DeepFace Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BjJL8dFe9t61",
    "outputId": "9d97fafa-4cf8-4cd0-ac72-08595a962cf3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read image\n",
    "data_raw_dir = \"/Users/tamirescorreamarcal/Desktop/Projetos/neuromovies/data_raw\"\n",
    "img_path = os.path.join(data_raw_dir, \"teste1.jpg\")\n",
    "image = cv2.imread(img_path)\n",
    "print(image.shape)\n",
    "\n",
    "# Display the image\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = DeepFace.analyze(\n",
    "            img_path=img_path,\n",
    "            actions=['age', 'gender', 'race', 'emotion'],\n",
    "            #xpand_percentage=0.2,\n",
    "            detector_backend='retinaface',\n",
    "            #anti_spoofing=True\n",
    "            #align=False\n",
    "           )\n",
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "JDOKb64Y9ITL",
    "outputId": "bad99768-74a8-45f0-fbd6-ccab73182018"
   },
   "outputs": [],
   "source": [
    "# Draw squares and emotions\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB\n",
    "for face in range(len(objs)):\n",
    "\n",
    "    # Draw rectangle around the face\n",
    "    x, y, w, h = objs[face]['region']['x'], objs[face]['region']['y'], objs[face]['region']['w'], objs[face]['region']['h']\n",
    "    \n",
    "    # Annotate emotion\n",
    "    emotion = objs[face]['dominant_emotion']\n",
    "    str0=\"Face Confidence: \" + str(objs[face]['face_confidence'])\n",
    "    str1=\"Emotion Confidence: \" + str(round(objs[face]['emotion'][emotion],0)/100)\n",
    "    str2=\"Emotion: \" + emotion\n",
    "    \n",
    "    # Image\n",
    "    height, width, _ = image_rgb.shape\n",
    "    font_size = width * 0.001           \n",
    "    thickness = int(width * 0.004)        \n",
    "    letters_distance = int(10*font_size) \n",
    "    color = (255, 0, 0)\n",
    "    image_labeled = cv2.rectangle(image_rgb, (x, y), (x+w, y+h), color, thickness)\n",
    "    image_labeled = cv2.rectangle(image_rgb, (x, y), (x+w, y+h), color, thickness)\n",
    "    cv2.putText(image_labeled, str0, (x+w+letters_distance, y+2*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "    cv2.putText(image_labeled, str1, (x+w+letters_distance, y+5*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "    cv2.putText(image_labeled, str2, (x+w+letters_distance, y+8*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_labeled)\n",
    "plt.show()\n",
    "print(str0)\n",
    "print(str1)\n",
    "print(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmyv_75_Q9w3",
    "outputId": "1bb2292c-32c1-421d-80ff-92e96d539d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': True,\n",
       " 'distance': 3.3306690738754696e-16,\n",
       " 'threshold': 0.4,\n",
       " 'model': 'Facenet',\n",
       " 'detector_backend': 'opencv',\n",
       " 'similarity_metric': 'cosine',\n",
       " 'facial_areas': {'img1': {'x': 313,\n",
       "   'y': 185,\n",
       "   'w': 415,\n",
       "   'h': 415,\n",
       "   'left_eye': (605, 348),\n",
       "   'right_eye': (432, 343)},\n",
       "  'img2': {'x': 313,\n",
       "   'y': 185,\n",
       "   'w': 415,\n",
       "   'h': 415,\n",
       "   'left_eye': (605, 348),\n",
       "   'right_eye': (432, 343)}},\n",
       " 'time': 1.38}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform face verification\n",
    "verification_result = DeepFace.verify(img_path, img_path, model_name=\"Facenet\", distance_metric=\"cosine\")\n",
    "verification_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4VFwVBYZIYf"
   },
   "source": [
    "### Video DeepFace Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_raw_dir = \"/Users/tamirescorreamarcal/Desktop/Projetos/neuromovies/data_raw\"\n",
    "data_processed_dir = \"/Users/tamirescorreamarcal/Desktop/Projetos/neuromovies/data_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euQNLgX6fbd0",
    "outputId": "6f246705-be64-4825-a6fe-3bfdb75e4f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video trailer10.mp4.\n",
      "MoviePy - Writing audio in trailer10TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video trailer10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready trailer10.mp4\n",
      "Frames per second and count: 10.0 and 1540\n",
      "Width: 640, Height: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Change Frame per Second\n",
    "name = \"barbiesmallest\"\n",
    "#name = \"trailer\"\n",
    "fps_ = 10\n",
    "cap = VideoFileClip(os.path.join(data_raw_dir,f\"{name}.mp4\"))\n",
    "cap = cap.set_fps(fps_)\n",
    "#cap = cap.subclip(\"00:17:20\", \"00:19:20\")\n",
    "cap.write_videofile(os.path.join(data_raw_dir,f\"{name}{fps_}.mp4\"), fps=fps_)\n",
    "\n",
    "# Check New Movie low Frame\n",
    "cap = cv2.VideoCapture(os.path.join(data_raw_dir,f\"{name}{fps_}.mp4\"))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"Frames per second and count: {fps} and {frame_count}\")\n",
    "print(f\"Width: {width}, Height: {height}\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second and count: 10.0 and 1200\n",
      "Width: 1920, Height: 960\n"
     ]
    }
   ],
   "source": [
    "# Check New Movie low Frame\n",
    "name = \"barbiesmallest10\"\n",
    "#name = \"trailer10\"\n",
    "cap = cv2.VideoCapture(os.path.join(data_raw_dir,f\"{name}.mp4\"))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"Frames per second and count: {fps} and {frame_count}\")\n",
    "print(f\"Width: {width}, Height: {height}\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lrkS3u3HZrJ0",
    "outputId": "705ce4c3-55bc-46d0-f394-375bf1c871f8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iteration over the movie to analyze faces and emotions\n",
    "1) Safe information in dataframe\n",
    "2) Show face analizes in each frame\n",
    "3) Save movie with face analizes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize video capture and writer\n",
    "name = \"barbiesmallest10\"\n",
    "#name = \"trailer10\"\n",
    "cap = cv2.VideoCapture(os.path.join(data_raw_dir,f\"{name}.mp4\"))\n",
    "cap_duplicate = cv2.VideoCapture(os.path.join(data_raw_dir,f\"{name}.mp4\"))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fps = 10\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')  # Codec for saving video\n",
    "out = cv2.VideoWriter(os.path.join(data_processed_dir,f\"processed_{name}.mp4\"),\n",
    "                      fourcc,\n",
    "                      fps, \n",
    "                      (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Iterate over the movie\n",
    "dict_face_times = {}\n",
    "second = 0\n",
    "frame_idx = 0\n",
    "last_frame_had_single_face = False\n",
    "face_match_prior = False\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame_idx = frame_idx + 1\n",
    "    second = frame_idx/fps # fps = 10\n",
    "\n",
    "    # Break the loop if we've reached the end of the video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Try to identify faces and emotions\n",
    "        objs = DeepFace.analyze(frame, \n",
    "                                actions=['emotion'], \n",
    "                                #xpand_percentage=0.2,\n",
    "                                detector_backend='retinaface',\n",
    "                                #anti_spoofing=True\n",
    "                                #align=False\n",
    "                               )\n",
    "        print(f\"Faces detected {frame_idx} frame of {frame_count} ({second} second)\")#: {objs}\")\n",
    "\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        if len(objs) == 1:\n",
    "\n",
    "            # Perform face verification for match prior frame\n",
    "            if last_frame_had_single_face:\n",
    "              cap_duplicate.set(cv2.CAP_PROP_POS_FRAMES, frame_idx-1)\n",
    "              _, prior_frame = cap_duplicate.read()\n",
    "              face_match_prior = DeepFace.verify(prior_frame, frame, model_name=\"Facenet\", distance_metric=\"cosine\")['verified']\n",
    "              #print(f\"Face match prior face in last frame: {face_match_prior}\")\n",
    "\n",
    "            # Save the emotions times for single faces\n",
    "            dict_face_times[frame_idx] = {\n",
    "                'second': second,\n",
    "                'type': \"single face\",\n",
    "                'face_confidence': objs[0]['face_confidence'],\n",
    "                'emotion': objs[0]['dominant_emotion'],\n",
    "                'emotion_confidence': round(objs[0]['emotion'][objs[0]['dominant_emotion']],0)/100,\n",
    "                'face_match_prior': face_match_prior}\n",
    "\n",
    "            # Save for next frame alanyzes\n",
    "            last_frame_had_single_face = True\n",
    "\n",
    "        elif len(objs) > 1:\n",
    "\n",
    "            # Save the times for multiple faces\n",
    "            dict_face_times[frame_idx] = {\n",
    "                'second': second,\n",
    "                'type': \"multiple faces\",\n",
    "                'face_confidence': np.nan,\n",
    "                'emotion': np.nan,\n",
    "                'emotion_confidence': np.nan,\n",
    "                'face_match_prior': np.nan}\n",
    "\n",
    "            # Save for next frame alanyzes\n",
    "            last_frame_had_single_face = False\n",
    "\n",
    "        #######################################################################\n",
    "        # Draw squares and emotions\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB\n",
    "        for face in range(len(objs)):\n",
    "\n",
    "          # Rectangle around the face\n",
    "          x, y, w, h = objs[face]['region']['x'], objs[face]['region']['y'], objs[face]['region']['w'], objs[face]['region']['h']\n",
    "\n",
    "          # Annotate emotion\n",
    "          emotion = objs[face]['dominant_emotion']\n",
    "          str0=\"Face Confidence: \" + str(objs[face]['face_confidence'])\n",
    "          str1=\"Emotion Confidence: \" + str(round(objs[face]['emotion'][emotion],0)/100)\n",
    "          str2=\"Emotion: \" + emotion\n",
    "          str3=\"* Single face match prior\"\n",
    "\n",
    "          # Image\n",
    "          width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "          font_size = width * 0.0007           \n",
    "          thickness = int(width * 0.003)        \n",
    "          letters_distance = int(10*font_size) \n",
    "          color = (255, 0, 0)\n",
    "          image_labeled = cv2.rectangle(image_rgb, (x, y), (x+w, y+h), color, thickness)\n",
    "          cv2.putText(image_labeled, str0, (x+w+letters_distance, y+2*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "          cv2.putText(image_labeled, str1, (x+w+letters_distance, y+5*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "          cv2.putText(image_labeled, str2, (x+w+letters_distance, y+8*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "          #if face_match_prior:\n",
    "          #  cv2.putText(image_labeled, str3, (x+w+letters_distance, y+11*letters_distance), cv2.FONT_HERSHEY_SIMPLEX, font_size, color, thickness)\n",
    "\n",
    "        # Display the image\n",
    "        plt.imshow(image_labeled)\n",
    "        plt.show()\n",
    "\n",
    "        # Convert to back to false\n",
    "        face_match_prior = False\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "    # Save the times for no faces\n",
    "    except Exception as e:\n",
    "        # Show image without faces\n",
    "        print(f\"Error processing frame {frame_idx} of {frame_count} ({second} second): {e}\")\n",
    "        image_labeled = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB\n",
    "        plt.imshow(image_labeled)\n",
    "        plt.show()\n",
    "        \n",
    "        dict_face_times[frame_idx] = {\n",
    "            'second': second,\n",
    "            'type': \"no faces\",\n",
    "            'face_confidence': np.nan,\n",
    "            'emotion': np.nan,\n",
    "            'emotion_confidence': np.nan,\n",
    "            'face_match_prior': np.nan}\n",
    "\n",
    "        # Save for next frame alanyzes\n",
    "        last_frame_had_single_face = False\n",
    "        \n",
    "\n",
    "    # Convert colour black\n",
    "    frame = cv2.cvtColor(image_labeled, cv2.COLOR_RGB2BGR)\n",
    "    # Write the frame with overlay to create labeled movie\n",
    "    out.write(frame)\n",
    "\n",
    "# Save dictonary as dataframe\n",
    "df_face_times = pd.DataFrame(dict_face_times).T\n",
    "df_face_times.to_csv(os.path.join(data_processed_dir, f\"df_face_times_{name}.csv\"))\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "cap_duplicate.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lR2P8zD9Yq5p",
    "90XPXP9IQ5AA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
